{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 64\n",
    "BATCH_SIZE = 128\n",
    "stats = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5) # mean, std for normalising images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: need to download files from http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html - data_dir must have following structure\n",
    "# .\n",
    "# ├── celeba\n",
    "# │   ├── identity_CelebA.txt\n",
    "# │   ├── img_align_celeba\n",
    "# │   │   ├── 000001.jpg\n",
    "# │   │   ...\n",
    "# │   │   └── 202599.jpg\n",
    "# │   ├── list_attr_celeba.txt\n",
    "# │   ├── list_bbox_celeba.txt\n",
    "# │   ├── list_eval_partition.txt\n",
    "# │   ├── list_landmarks_align_celeba.txt\n",
    "# │   ├── list_landmarks_celeba.txt\n",
    "# │   └── README.txt\n",
    "\n",
    "data_dir = pathlib.Path.cwd().parent/'data'\n",
    "transforms = T.Compose([T.Resize(IMG_SIZE),\n",
    "                        T.CenterCrop(IMG_SIZE), #  Central square crop\n",
    "                        T.ToTensor(),\n",
    "                        T.Normalize(*stats) # normalize => mean 0 std 1                              \n",
    "                        ])       \n",
    "celeb_data = torchvision.datasets.CelebA(data_dir, split = 'all', transform = transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "celeb_dl = torch.utils.data.DataLoader(celeb_data, BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denorm(img_tensors):\n",
    "    \"Denormalize image tensor with specified mean and std\"\n",
    "    return img_tensors * stats[1][0] + stats[0][0]\n",
    "\n",
    "def show_images(images, nmax=64):\n",
    "  fig, ax = plt.subplots(figsize=(8,8))\n",
    "  ax.set_xticks([]); ax.set_yticks([])\n",
    "  ax.imshow(torchvision.utils.make_grid(denorm(images.detach()[:nmax]), nrow=8).permute(1, 2, 0))\n",
    "  \n",
    "def show_batch(dl, nmax=64):\n",
    "  for images, _ in dl:\n",
    "    show_images(images, nmax)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_batch(celeb_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build GAN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move dataloader to GPU (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    \"\"\" 3 things:\n",
    "    1. Connected to Nvidia GPU\n",
    "    2. Cuda drivers\n",
    "    3. Pytorch suitable to GPU version\n",
    "    then torch.cuda.is_available is True\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "def to_device(data, device):\n",
    "  \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "  if isinstance(data, (list,tuple)):\n",
    "      return [to_device(x, device) for x in data]\n",
    "  return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_default_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DeviceDataLoader(celeb_dl, device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = nn.Sequential(\n",
    "    # in: 3x 64 x 64\n",
    "    nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.LeakyReLU(0.2, inplace=True),\n",
    "    # out: 64 x 32 x 32\n",
    "\n",
    "    nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(128),\n",
    "    nn.LeakyReLU(0.2, inplace=True),\n",
    "    # out: 128 x 16 x 16\n",
    "\n",
    "    nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(256),\n",
    "    nn.LeakyReLU(0.2, inplace=True),\n",
    "    # out: 256 x 8 x 8\n",
    "\n",
    "    nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(512),\n",
    "    nn.LeakyReLU(0.2, inplace=True),\n",
    "    # out: 512 x 4 x 4\n",
    "\n",
    "    nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "    # out: 1 x 1 x 1\n",
    "\n",
    "    nn.Flatten(),\n",
    "    nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = to_device(discriminator, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tensor Batch_Size,C,H,W\n",
    "X = torch.rand(size=(1, 3, 64, 64), dtype=torch.float32, device=device) \n",
    "for layer in discriminator:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__,'output shape: \\t', X.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = nn.Sequential(\n",
    "    # in: latent_size x 1 x 1\n",
    "\n",
    "    nn.ConvTranspose2d(LATENT_SIZE, 512, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "    nn.BatchNorm2d(512),\n",
    "    nn.ReLU(True),\n",
    "    # out: 512 x 4 x 4\n",
    "\n",
    "    nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(256),\n",
    "    nn.ReLU(True),\n",
    "    # out: 256 x 8 x 8\n",
    "\n",
    "    nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(128),\n",
    "    nn.ReLU(True),\n",
    "    # out: 128 x 16 x 16\n",
    "\n",
    "    nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(True),\n",
    "    # out: 64 x 32 x 32\n",
    "\n",
    "    nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    nn.Tanh()  # output is between -1 to 1\n",
    "    # out: 3 x 64 x 64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(size=(1, 128, 1, 1))\n",
    "for layer in generator:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__,'output shape: \\t',X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb = torch.randn(BATCH_SIZE, LATENT_SIZE, 1, 1) # random latent tensors\n",
    "fake_images = generator(xb)\n",
    "print(fake_images.shape)\n",
    "show_images(fake_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = to_device(generator, device) # move generator to device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_discriminator(real_images, opt_d):\n",
    "  # Clear discriminator gradients\n",
    "  opt_d.zero_grad()\n",
    "\n",
    "  # Pass real images through  discriminator\n",
    "  real_preds = discriminator(real_images)\n",
    "  real_targets = torch.ones(real_images.size(0), 1, device=device)\n",
    "  real_loss = F.binary_cross_entropy(real_preds, real_targets)\n",
    "  real_score = torch.mean(real_preds).item()\n",
    "\n",
    "  # Generate fake images\n",
    "  latent = torch.randn(BATCH_SIZE, LATENT_SIZE, 1, 1, device=device)\n",
    "  fake_images = generator(latent)\n",
    "\n",
    "  # Pass Fake images through discriminator\n",
    "  fake_targets = torch.zeros(fake_images.size(0), 1, device=device)\n",
    "  fake_preds = discriminator(fake_images)\n",
    "  fake_loss = F.binary_cross_entropy(fake_preds, fake_targets)\n",
    "  fake_score = torch.mean(fake_preds).item()\n",
    "\n",
    "  # Update discriminator weights\n",
    "  loss = real_loss + fake_loss\n",
    "  loss.backward()\n",
    "  opt_d.step()\n",
    "  return loss.item(), real_score, fake_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(opt_g):\n",
    "  # Clear generator gradients\n",
    "  opt_g.zero_grad()\n",
    "\n",
    "  # Generate fake images\n",
    "  latent = torch.randn(BATCH_SIZE, LATENT_SIZE, 1,1, device=device)\n",
    "  fake_images = generator(latent)\n",
    "\n",
    "  # Try to fool the discriminator\n",
    "  preds = discriminator(fake_images)\n",
    "  targets = torch.ones(BATCH_SIZE, 1, device=device)\n",
    "  loss = F.binary_cross_entropy(preds, targets)\n",
    "\n",
    "  # Update generator \n",
    "  loss.backward()\n",
    "  opt_g.step()\n",
    "\n",
    "  return loss.item()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create intermediate output folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dir = data_dir / 'generated'\n",
    "os.makedirs(sample_dir, exist_ok=True)\n",
    "\n",
    "def save_samples(index, latent_tensors, show=True):\n",
    "  fake_images = generator(latent_tensors)\n",
    "  fake_fname = 'generated=images-{0:0=4d}.png'.format(index)\n",
    "  torchvision.utils.save_image(denorm(fake_images), os.path.join(sample_dir, fake_fname), nrow=8)\n",
    "  print(\"Saving\", fake_fname)\n",
    "\n",
    "  if show:\n",
    "    fig, ax = plt.subplots(figsize=(8,8))\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "    ax.imshow(torchvision.utils.make_grid(fake_images.cpu().detach(), nrow=8).permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_latent = torch.randn(64, LATENT_SIZE, 1, 1, device=device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_samples(0, fixed_latent)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "LR = 0.00025\n",
    "EPOCHS = 60;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkpoint(model, model_dir, filename):\n",
    "    torch.save(model.state_dict(), model_dir/filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = pathlib.Path.cwd().parent / 'models'\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, lr, checkpoint_dir, start_idx = 1):\n",
    "  torch.cuda.empty_cache()\n",
    "\n",
    "  # Losses & scores\n",
    "  losses_g = []\n",
    "  losses_d = []\n",
    "  real_scores = []\n",
    "  fake_scores = []\n",
    "\n",
    "  # Create optimizers\n",
    "  opt_d = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "  opt_g = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    for real_images, _ in tqdm(train_dl):\n",
    "      # Train discriminator\n",
    "      loss_d, real_score, fake_score = train_discriminator(real_images, opt_d)\n",
    "      # Train generator\n",
    "      loss_g = train_generator(opt_g)\n",
    "\n",
    "    # Record losses & scores\n",
    "    losses_g.append(loss_g)\n",
    "    losses_d.append(loss_d)\n",
    "    real_scores.append(real_score)\n",
    "    fake_scores.append(fake_score)\n",
    "\n",
    "    # Log losses & scores (last batch)\n",
    "    print(\"Epoch [{}/{}], loss_g: {:.4f}, loss_d: {:.4f}, real_score: {:.4f}, fake_score: {:.4f}\".format(epoch+1, epochs, loss_g, loss_d, real_score, fake_score))\n",
    "    # Save model weights and generated images\n",
    "    checkpoint(generator, checkpoint_dir/f'Generator_{epoch+1}')\n",
    "    checkpoint(discriminator, checkpoint_dir/f'Discriminator_{epoch+1}')\n",
    "    save_samples(epoch+start_idx, fixed_latent, show=False)\n",
    "    \n",
    "\n",
    "  return losses_g, losses_d, real_scores, fake_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = fit(EPOCHS, LR, model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsgan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
