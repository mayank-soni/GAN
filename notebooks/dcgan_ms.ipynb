{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMG_SIZE = 64\n",
    "BATCH_SIZE = 128\n",
    "stats = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5) # mean, std for normalising images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 22\u001b[0m\n\u001b[1;32m     16\u001b[0m data_dir \u001b[39m=\u001b[39m pathlib\u001b[39m.\u001b[39mPath\u001b[39m.\u001b[39mcwd()\u001b[39m.\u001b[39mparent\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     17\u001b[0m transforms \u001b[39m=\u001b[39m T\u001b[39m.\u001b[39mCompose([T\u001b[39m.\u001b[39mResize(IMG_SIZE),\n\u001b[1;32m     18\u001b[0m                         T\u001b[39m.\u001b[39mCenterCrop(IMG_SIZE), \u001b[39m#  Central square crop\u001b[39;00m\n\u001b[1;32m     19\u001b[0m                         T\u001b[39m.\u001b[39mToTensor(),\n\u001b[1;32m     20\u001b[0m                         T\u001b[39m.\u001b[39mNormalize(\u001b[39m*\u001b[39mstats) \u001b[39m# normalize => mean 0 std 1                              \u001b[39;00m\n\u001b[1;32m     21\u001b[0m                         ])       \n\u001b[0;32m---> 22\u001b[0m celeb_data \u001b[39m=\u001b[39m torchvision\u001b[39m.\u001b[39;49mdatasets\u001b[39m.\u001b[39;49mCelebA(data_dir, split \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mall\u001b[39;49m\u001b[39m'\u001b[39;49m, transform \u001b[39m=\u001b[39;49m transforms)\n",
      "File \u001b[0;32m~/anaconda3/envs/dsgan/lib/python3.8/site-packages/torchvision/datasets/celeba.py:92\u001b[0m, in \u001b[0;36mCelebA.__init__\u001b[0;34m(self, root, split, target_type, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     85\u001b[0m split_map \u001b[39m=\u001b[39m {\n\u001b[1;32m     86\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m0\u001b[39m,\n\u001b[1;32m     87\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mvalid\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m1\u001b[39m,\n\u001b[1;32m     88\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m2\u001b[39m,\n\u001b[1;32m     89\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     90\u001b[0m }\n\u001b[1;32m     91\u001b[0m split_ \u001b[39m=\u001b[39m split_map[verify_str_arg(split\u001b[39m.\u001b[39mlower(), \u001b[39m\"\u001b[39m\u001b[39msplit\u001b[39m\u001b[39m\"\u001b[39m, (\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mvalid\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m))]\n\u001b[0;32m---> 92\u001b[0m splits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load_csv(\u001b[39m\"\u001b[39;49m\u001b[39mlist_eval_partition.txt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     93\u001b[0m identity \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_csv(\u001b[39m\"\u001b[39m\u001b[39midentity_CelebA.txt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     94\u001b[0m bbox \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_csv(\u001b[39m\"\u001b[39m\u001b[39mlist_bbox_celeba.txt\u001b[39m\u001b[39m\"\u001b[39m, header\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/dsgan/lib/python3.8/site-packages/torchvision/datasets/celeba.py:118\u001b[0m, in \u001b[0;36mCelebA._load_csv\u001b[0;34m(self, filename, header)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_load_csv\u001b[39m(\n\u001b[1;32m    113\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    114\u001b[0m     filename: \u001b[39mstr\u001b[39m,\n\u001b[1;32m    115\u001b[0m     header: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    116\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m CSV:\n\u001b[1;32m    117\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_folder, filename)) \u001b[39mas\u001b[39;00m csv_file:\n\u001b[0;32m--> 118\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(csv\u001b[39m.\u001b[39;49mreader(csv_file, delimiter\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\"\u001b[39;49m, skipinitialspace\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m))\n\u001b[1;32m    120\u001b[0m     \u001b[39mif\u001b[39;00m header \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m         headers \u001b[39m=\u001b[39m data[header]\n",
      "File \u001b[0;32m~/anaconda3/envs/dsgan/lib/python3.8/codecs.py:319\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_buffer_decode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, errors, final):\n\u001b[1;32m    315\u001b[0m     \u001b[39m# Overwrite this method in subclasses: It must decode input\u001b[39;00m\n\u001b[1;32m    316\u001b[0m     \u001b[39m# and return an (output, length consumed) tuple\u001b[39;00m\n\u001b[1;32m    317\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m\n\u001b[0;32m--> 319\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    320\u001b[0m     \u001b[39m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[1;32m    321\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer \u001b[39m+\u001b[39m \u001b[39minput\u001b[39m\n\u001b[1;32m    322\u001b[0m     (result, consumed) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer_decode(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39merrors, final)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Note: need to download files from http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html - data_dir must have following structure\n",
    "# .\n",
    "# ├── celeba\n",
    "# │   ├── identity_CelebA.txt\n",
    "# │   ├── img_align_celeba\n",
    "# │   │   ├── 000001.jpg\n",
    "# │   │   ...\n",
    "# │   │   └── 202599.jpg\n",
    "# │   ├── list_attr_celeba.txt\n",
    "# │   ├── list_bbox_celeba.txt\n",
    "# │   ├── list_eval_partition.txt\n",
    "# │   ├── list_landmarks_align_celeba.txt\n",
    "# │   ├── list_landmarks_celeba.txt\n",
    "# │   └── README.txt\n",
    "\n",
    "data_dir = pathlib.Path.cwd().parent/'data'\n",
    "transforms = T.Compose([T.Resize(IMG_SIZE),\n",
    "                        T.CenterCrop(IMG_SIZE), #  Central square crop\n",
    "                        T.ToTensor(),\n",
    "                        T.Normalize(*stats) # normalize => mean 0 std 1                              \n",
    "                        ])       \n",
    "celeb_data = torchvision.datasets.CelebA(data_dir, split = 'all', transform = transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "celeb_dl = torch.utils.data.DataLoader(celeb_data, BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denorm(img_tensors):\n",
    "    \"Denormalize image tensor with specified mean and std\"\n",
    "    return img_tensors * stats[1][0] + stats[0][0]\n",
    "\n",
    "def show_images(images, nmax=64):\n",
    "  fig, ax = plt.subplots(figsize=(8,8))\n",
    "  ax.set_xticks([]); ax.set_yticks([])\n",
    "  ax.imshow(torchvision.utils.make_grid(denorm(images.detach()[:nmax]), nrow=8).permute(1, 2, 0))\n",
    "  \n",
    "def show_batch(dl, nmax=64):\n",
    "  for images, _ in dl:\n",
    "    show_images(images, nmax)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_batch(celeb_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build GAN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move dataloader to GPU (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    \"\"\" 3 things:\n",
    "    1. Connected to Nvidia GPU\n",
    "    2. Cuda drivers\n",
    "    3. Pytorch suitable to GPU version\n",
    "    then torch.cuda.is_available is True\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "def to_device(data, device):\n",
    "  \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "  if isinstance(data, (list,tuple)):\n",
    "      return [to_device(x, device) for x in data]\n",
    "  return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_default_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DeviceDataLoader(celeb_dl, device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = nn.Sequential(\n",
    "    # in: 3x 64 x 64\n",
    "    nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.LeakyReLU(0.2, inplace=True),\n",
    "    # out: 64 x 32 x 32\n",
    "\n",
    "    nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(128),\n",
    "    nn.LeakyReLU(0.2, inplace=True),\n",
    "    # out: 128 x 16 x 16\n",
    "\n",
    "    nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(256),\n",
    "    nn.LeakyReLU(0.2, inplace=True),\n",
    "    # out: 256 x 8 x 8\n",
    "\n",
    "    nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(512),\n",
    "    nn.LeakyReLU(0.2, inplace=True),\n",
    "    # out: 512 x 4 x 4\n",
    "\n",
    "    nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "    # out: 1 x 1 x 1\n",
    "\n",
    "    nn.Flatten(),\n",
    "    nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = to_device(discriminator, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tensor Batch_Size,C,H,W\n",
    "X = torch.rand(size=(1, 3, 64, 64), dtype=torch.float32, device=device) \n",
    "for layer in discriminator:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__,'output shape: \\t', X.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = nn.Sequential(\n",
    "    # in: latent_size x 1 x 1\n",
    "\n",
    "    nn.ConvTranspose2d(LATENT_SIZE, 512, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "    nn.BatchNorm2d(512),\n",
    "    nn.ReLU(True),\n",
    "    # out: 512 x 4 x 4\n",
    "\n",
    "    nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(256),\n",
    "    nn.ReLU(True),\n",
    "    # out: 256 x 8 x 8\n",
    "\n",
    "    nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(128),\n",
    "    nn.ReLU(True),\n",
    "    # out: 128 x 16 x 16\n",
    "\n",
    "    nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(True),\n",
    "    # out: 64 x 32 x 32\n",
    "\n",
    "    nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    nn.Tanh()  # output is between -1 to 1\n",
    "    # out: 3 x 64 x 64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(size=(1, 128, 1, 1))\n",
    "for layer in generator:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__,'output shape: \\t',X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb = torch.randn(BATCH_SIZE, LATENT_SIZE, 1, 1) # random latent tensors\n",
    "fake_images = generator(xb)\n",
    "print(fake_images.shape)\n",
    "show_images(fake_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = to_device(generator, device) # move generator to device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_discriminator(real_images, opt_d):\n",
    "  # Clear discriminator gradients\n",
    "  opt_d.zero_grad()\n",
    "\n",
    "  # Pass real images through  discriminator\n",
    "  real_preds = discriminator(real_images)\n",
    "  real_targets = torch.ones(real_images.size(0), 1, device=device)\n",
    "  real_loss = F.binary_cross_entropy(real_preds, real_targets)\n",
    "  real_score = torch.mean(real_preds).item()\n",
    "\n",
    "  # Generate fake images\n",
    "  latent = torch.randn(BATCH_SIZE, LATENT_SIZE, 1, 1, device=device)\n",
    "  fake_images = generator(latent)\n",
    "\n",
    "  # Pass Fake images through discriminator\n",
    "  fake_targets = torch.zeros(fake_images.size(0), 1, device=device)\n",
    "  fake_preds = discriminator(fake_images)\n",
    "  fake_loss = F.binary_cross_entropy(fake_preds, fake_targets)\n",
    "  fake_score = torch.mean(fake_preds).item()\n",
    "\n",
    "  # Update discriminator weights\n",
    "  loss = real_loss + fake_loss\n",
    "  loss.backward()\n",
    "  opt_d.step()\n",
    "  return loss.item(), real_score, fake_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(opt_g):\n",
    "  # Clear generator gradients\n",
    "  opt_g.zero_grad()\n",
    "\n",
    "  # Generate fake images\n",
    "  latent = torch.randn(BATCH_SIZE, LATENT_SIZE, 1,1, device=device)\n",
    "  fake_images = generator(latent)\n",
    "\n",
    "  # Try to fool the discriminator\n",
    "  preds = discriminator(fake_images)\n",
    "  targets = torch.ones(BATCH_SIZE, 1, device=device)\n",
    "  loss = F.binary_cross_entropy(preds, targets)\n",
    "\n",
    "  # Update generator \n",
    "  loss.backward()\n",
    "  opt_g.step()\n",
    "\n",
    "  return loss.item()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create intermediate output folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dir = data_dir / 'generated'\n",
    "os.makedirs(sample_dir, exist_ok=True)\n",
    "\n",
    "def save_samples(index, latent_tensors, show=True):\n",
    "  fake_images = generator(latent_tensors)\n",
    "  fake_fname = 'generated=images-{0:0=4d}.png'.format(index)\n",
    "  torchvision.utils.save_image(denorm(fake_images), os.path.join(sample_dir, fake_fname), nrow=8)\n",
    "  print(\"Saving\", fake_fname)\n",
    "\n",
    "  if show:\n",
    "    fig, ax = plt.subplots(figsize=(8,8))\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "    ax.imshow(torchvision.utils.make_grid(fake_images.cpu().detach(), nrow=8).permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_latent = torch.randn(64, LATENT_SIZE, 1, 1, device=device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_samples(0, fixed_latent)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "LR = 0.00025\n",
    "EPOCHS = 60;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkpoint(model, model_dir, filename):\n",
    "    torch.save(model.state_dict(), model_dir/filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = pathlib.Path.cwd().parent / 'models'\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, lr, checkpoint_dir, start_idx = 1):\n",
    "  torch.cuda.empty_cache()\n",
    "\n",
    "  # Losses & scores\n",
    "  losses_g = []\n",
    "  losses_d = []\n",
    "  real_scores = []\n",
    "  fake_scores = []\n",
    "\n",
    "  # Create optimizers\n",
    "  opt_d = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "  opt_g = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    for real_images, _ in tqdm(train_dl):\n",
    "      # Train discriminator\n",
    "      loss_d, real_score, fake_score = train_discriminator(real_images, opt_d)\n",
    "      # Train generator\n",
    "      loss_g = train_generator(opt_g)\n",
    "\n",
    "    # Record losses & scores\n",
    "    losses_g.append(loss_g)\n",
    "    losses_d.append(loss_d)\n",
    "    real_scores.append(real_score)\n",
    "    fake_scores.append(fake_score)\n",
    "\n",
    "    # Log losses & scores (last batch)\n",
    "    print(\"Epoch [{}/{}], loss_g: {:.4f}, loss_d: {:.4f}, real_score: {:.4f}, fake_score: {:.4f}\".format(epoch+1, epochs, loss_g, loss_d, real_score, fake_score))\n",
    "    # Save model weights and generated images\n",
    "    checkpoint(generator, checkpoint_dir, f'Generator_{epoch+1}')\n",
    "    checkpoint(discriminator, checkpoint_dir, f'Discriminator_{epoch+1}')\n",
    "    save_samples(epoch+start_idx, fixed_latent, show=False)\n",
    "    \n",
    "\n",
    "  return losses_g, losses_d, real_scores, fake_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = fit(EPOCHS, LR, model_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsgan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
